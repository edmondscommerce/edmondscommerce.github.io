<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Server on Edmonds Commerce Dev Blog</title>
    <link>https://edmondscommerce.github.io/tags/server/</link>
    <description>Recent content in Server on Edmonds Commerce Dev Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 07 May 2015 10:32:04 +0000</lastBuildDate>
    <atom:link href="https://edmondscommerce.github.io/tags/server/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Running out of disk space but df shows you have lots of space left</title>
      <link>https://edmondscommerce.github.io/linux/vps/server/running-out-of-disk-space-but-df-shows-you-have-lots-of-space-left.html</link>
      <pubDate>Thu, 07 May 2015 10:32:04 +0000</pubDate>
      
      <guid>https://edmondscommerce.github.io/linux/vps/server/running-out-of-disk-space-but-df-shows-you-have-lots-of-space-left.html</guid>
      <description>&lt;p&gt;Had a strange occurency this morning with an server reporting it had no disk space when disk usage according to df was at 50%. After some head scratching I decided to read the man page for the df command and came across the -i flag. According the man page this lists inode information instead of block usage. After running it I got the following:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span&gt;&lt;/span&gt;Filesystem      Inodes   IUsed  IFree IUse% Mounted on
/dev/xvda1     &lt;span class=&#34;m&#34;&gt;2621440&lt;/span&gt; &lt;span class=&#34;m&#34;&gt;2621440&lt;/span&gt;      &lt;span class=&#34;m&#34;&gt;0&lt;/span&gt;  100% /
udev            &lt;span class=&#34;m&#34;&gt;124879&lt;/span&gt;     &lt;span class=&#34;m&#34;&gt;398&lt;/span&gt; &lt;span class=&#34;m&#34;&gt;124481&lt;/span&gt;    1% /dev
tmpfs           &lt;span class=&#34;m&#34;&gt;126892&lt;/span&gt;     &lt;span class=&#34;m&#34;&gt;288&lt;/span&gt; &lt;span class=&#34;m&#34;&gt;126604&lt;/span&gt;    1% /run
none            &lt;span class=&#34;m&#34;&gt;126892&lt;/span&gt;       &lt;span class=&#34;m&#34;&gt;4&lt;/span&gt; &lt;span class=&#34;m&#34;&gt;126888&lt;/span&gt;    1% /run/lock
none            &lt;span class=&#34;m&#34;&gt;126892&lt;/span&gt;       &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt; &lt;span class=&#34;m&#34;&gt;126891&lt;/span&gt;    1% /run/shm
none            &lt;span class=&#34;m&#34;&gt;126892&lt;/span&gt;       &lt;span class=&#34;m&#34;&gt;1&lt;/span&gt; &lt;span class=&#34;m&#34;&gt;126891&lt;/span&gt;    1% /run/user
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;So it turned out that the system had reached its max inodes so the next line of investigation was to find any excess of files in an directory. Which was sone by using:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; i in /var/*&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;do&lt;/span&gt; &lt;span class=&#34;nb&#34;&gt;echo&lt;/span&gt; &lt;span class=&#34;nv&#34;&gt;$i&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; find &lt;span class=&#34;nv&#34;&gt;$i&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;|&lt;/span&gt; wc -l&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;done&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This will list the total number of files in a given directory. You can change the path to work your way down the directory tree finding directories with lots of files. In the end we found excess of old logs which I was able to remove.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>SSL testing and SHA-1 Sunsetting</title>
      <link>https://edmondscommerce.github.io/security/ssl-testing-and-sha-1-sunsetting.html</link>
      <pubDate>Wed, 12 Nov 2014 13:03:36 +0000</pubDate>
      
      <guid>https://edmondscommerce.github.io/security/ssl-testing-and-sha-1-sunsetting.html</guid>
      <description>&lt;p&gt;Over last few days we been doing a bit of work on doing some audits for people and one of the things that has come up is ssl. For the purpose of checking the servers ssl implementation we have been using &lt;a href=&#34;https://www.ssllabs.com/ssltest/&#34; target=&#34;_blank&#34;&gt;Qualys SSL Labs&lt;/a&gt; which is a excelent tool.&lt;/p&gt;

&lt;p&gt;The checker checks for the ssl certificate and tells you if your server setup is vulnerable to attacks such as Poodle and OpenSSL CCS Injection. We will produce a blog post shortly on the best practise setup to prevent these attcks.&lt;/p&gt;

&lt;p&gt;For the moment SHA-1 as a certificate signature algorithm is getting depricated in &lt;a href=&#34;https://community.qualys.com/blogs/securitylabs/2014/09/09/sha1-deprecation-what-you-need-to-know&#34; target=&#34;_blank&#34;&gt;chrome&lt;/a&gt;. As the cost for collision attcks against SHA-1 will become more &lt;a href=&#34;https://www.schneier.com/blog/archives/2012/10/when_will_we_se.html&#34; target=&#34;_blank&#34;&gt;affordable&lt;/a&gt; in the next few years.&lt;/p&gt;

&lt;p&gt;Goole have set the cut off for certificates after 2016 but we will be seeing some crosses on the padlock in chrom during the first quater of 2015.&lt;/p&gt;

&lt;p&gt;If your certificate expires in 2015 then you will not see any chrome. If your certificate expires in 2016 you will see some minor errors being reported in chrome. However if your certificate expires in 2017 then chrome will treat it as an insecure certificate in 2015.&lt;/p&gt;

&lt;p&gt;So our advice is:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;If your certificate expires in 2015: When you come to renew make sure you get SHA-256 as the certificate signature&lt;/li&gt;
&lt;li&gt;If your certificate expires in 2016: Think about getting renewed during 2015 or earlier.&lt;/li&gt;
&lt;li&gt;If your certificate expires in 2017: Think about getting a new one issue as soon as possible.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Using Apache JMeter For Load Testing</title>
      <link>https://edmondscommerce.github.io/load-testing/using-apache-jmeter-for-load-testing.html</link>
      <pubDate>Tue, 27 May 2014 13:01:39 +0000</pubDate>
      
      <guid>https://edmondscommerce.github.io/load-testing/using-apache-jmeter-for-load-testing.html</guid>
      <description>&lt;p&gt;Load testing is something that you really need a decent solution for. A simple tool such as Apache Bench (ab) is overly simplistic for todays web applications.&lt;/p&gt;

&lt;p&gt;After some research I decided to use Apache Jmeter as the tool of choice. It is written in Java so easy enough to run on any platform.
It features a GUI which is ideal for creating your tests, then a command line version which is what you should use to run your tests.&lt;/p&gt;

&lt;p&gt;For my purposes I wanted to feed in a large list of URLs, for that I followed these instructions:
&lt;a href=&#34;http://asciiville.com/musings/coder/how-to-feed-jmeter-from-csv&#34; target=&#34;_blank&#34;&gt;http://asciiville.com/musings/coder/how-to-feed-jmeter-from-csv&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This allows you to create a csv file with your choice of URLs. It was actually quite hard to find a decent and succint guide to getting this set up but thankfully that page fitted my requirements.&lt;/p&gt;

&lt;p&gt;Once I started to run the test I quickly realised that my system was grinding to a halt with out of memory errors. A bit more searching yielded this page which advises on how to properly run Jmeter including the fact that you really should run it on the command line for proper testing.
&lt;a href=&#34;http://blazemeter.com/blog/jmeter-performance-and-tuning-tips&#34; target=&#34;_blank&#34;&gt;http://blazemeter.com/blog/jmeter-performance-and-tuning-tips&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Prestashop Debug Mode using the Magento Environment Variable Method</title>
      <link>https://edmondscommerce.github.io/prestashop/prestashop-debug-mode-using-the-magento-environment-variable-method.html</link>
      <pubDate>Thu, 25 Apr 2013 10:14:30 +0000</pubDate>
      
      <guid>https://edmondscommerce.github.io/prestashop/prestashop-debug-mode-using-the-magento-environment-variable-method.html</guid>
      <description>&lt;p&gt;Magento developers are no doubt familiar with the concept of defining a server environment variable MAGE_IS_DEVELOPER_MODE so that when running Magento locally it is always in developer mode but there is no chance of accidently deploying a live site in developer mode.&lt;/p&gt;

&lt;p&gt;This little trick will allow you to use the same environment variable to also have the same effect when working with PrestaShop.&lt;/p&gt;

&lt;p&gt;To explain - developer mode generally means that things like error messages and debugging information are displayed clearly in the browser which makes things much easier when you are developing, to see what is going wrong.&lt;/p&gt;

&lt;p&gt;On a live site you want to keep error information hidden and generally display to the customer a more friendly error message screen that perhaps helps them find their way back to a functional section of the site or gives them information to contact the site owner directly for assistance.&lt;/p&gt;

&lt;p&gt;Anyway, to put PrestaShop in debug mode locally but have no risk of deployign this to live you can alter the file: config/defines.inc.php&lt;/p&gt;

&lt;p&gt;from&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;code class=&#34;language-php&#34; data-lang=&#34;php&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&#34;x&#34;&gt;define(&amp;#39;_PS_MODE_DEV_&amp;#39;, false);&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;to&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;code class=&#34;language-php&#34; data-lang=&#34;php&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&#34;x&#34;&gt;define(&amp;#39;_PS_MODE_DEV_&amp;#39;, isset($_SERVER[&amp;#39;MAGE_IS_DEVELOPER_MODE&amp;#39;]));&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Apache Log File Analysis Script</title>
      <link>https://edmondscommerce.github.io/bash/apache-log-file-analysis-script.html</link>
      <pubDate>Thu, 14 Mar 2013 14:17:48 +0000</pubDate>
      
      <guid>https://edmondscommerce.github.io/bash/apache-log-file-analysis-script.html</guid>
      <description>&lt;p&gt;Here is a little bash script we knocked together to track down some malicious activity on a clients server.&lt;/p&gt;

&lt;p&gt;Using a bit of awk etc to parse the log files we could quickly track down an IP address that was overloading the server and then take steps to block that person.&lt;/p&gt;

&lt;p&gt;Here is the script:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&#34;ch&#34;&gt;#!/bin/bash&lt;/span&gt;

&lt;span class=&#34;c1&#34;&gt;###### SETUP ############&lt;/span&gt;
&lt;span class=&#34;nv&#34;&gt;LOG_FOLDER&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;/var/www/vhosts/domain.co.uk/statistics/logs
&lt;span class=&#34;nv&#34;&gt;ACCESS_LOG&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;$LOG_FOLDER&lt;/span&gt;/access_log

&lt;span class=&#34;nv&#34;&gt;HOW_MANY_ROWS&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;20000



&lt;span class=&#34;c1&#34;&gt;######### FUNCTIONS ##############&lt;/span&gt;


&lt;span class=&#34;k&#34;&gt;function&lt;/span&gt; title&lt;span class=&#34;o&#34;&gt;()&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;{&lt;/span&gt;
    &lt;span class=&#34;nb&#34;&gt;echo&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;quot;&lt;/span&gt;
&lt;span class=&#34;s2&#34;&gt;---------------------------------&lt;/span&gt;
&lt;span class=&#34;nv&#34;&gt;$@&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&lt;/span&gt;
&lt;span class=&#34;s2&#34;&gt;---------------------------------&lt;/span&gt;
&lt;span class=&#34;s2&#34;&gt;&amp;quot;&lt;/span&gt;
&lt;span class=&#34;o&#34;&gt;}&lt;/span&gt;

&lt;span class=&#34;k&#34;&gt;function&lt;/span&gt; urls_by_ip&lt;span class=&#34;o&#34;&gt;()&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;{&lt;/span&gt;
    &lt;span class=&#34;nb&#34;&gt;local&lt;/span&gt; &lt;span class=&#34;nv&#34;&gt;IP&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;$1&lt;/span&gt;
    tail -5000 &lt;span class=&#34;nv&#34;&gt;$ACCESS_LOG&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;|&lt;/span&gt; awk -v &lt;span class=&#34;nv&#34;&gt;ip&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;$IP&lt;/span&gt; &lt;span class=&#34;s1&#34;&gt;&amp;#39; $1 ~ ip {freq[$7]++} END {for (x in freq) {print freq[x], x}}&amp;#39;&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;|&lt;/span&gt; sort -rn &lt;span class=&#34;p&#34;&gt;|&lt;/span&gt; head -20
&lt;span class=&#34;o&#34;&gt;}&lt;/span&gt;


&lt;span class=&#34;k&#34;&gt;function&lt;/span&gt; ip_addresses_by_user_agent&lt;span class=&#34;o&#34;&gt;(){&lt;/span&gt;
    &lt;span class=&#34;nb&#34;&gt;local&lt;/span&gt; &lt;span class=&#34;nv&#34;&gt;USERAGENT_STRING&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;$1&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;&lt;/span&gt;
    &lt;span class=&#34;nb&#34;&gt;local&lt;/span&gt; &lt;span class=&#34;nv&#34;&gt;TOP_20_IPS&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;`tail  -&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;$HOW_MANY_ROWS&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;$ACCESS_LOG&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt; | grep &amp;quot;&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;${&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;USERAGENT_STRING&lt;/span&gt;&lt;span class=&#34;si&#34;&gt;}&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;  | awk &amp;#39;{freq[&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;$1&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;]++} END {for (x in freq) {print freq[x], x}}&amp;#39; | sort -rn | head -20`&amp;quot;&lt;/span&gt;
    &lt;span class=&#34;nb&#34;&gt;echo&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;$TOP_20_IPS&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;&lt;/span&gt;
&lt;span class=&#34;o&#34;&gt;}&lt;/span&gt;

&lt;span class=&#34;c1&#34;&gt;####### RUN REPORTS #############&lt;/span&gt;


title &lt;span class=&#34;s2&#34;&gt;&amp;quot;top 20 URLs&amp;quot;&lt;/span&gt;
&lt;span class=&#34;nv&#34;&gt;TOP_20_URLS&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;`tail -&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;$HOW_MANY_ROWS&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;$ACCESS_LOG&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt; | awk &amp;#39;{freq[&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;$7&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;]++} END {for (x in freq) {print freq[x], x}}&amp;#39; | sort -rn | head -20`&amp;quot;&lt;/span&gt;
&lt;span class=&#34;nb&#34;&gt;echo&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;$TOP_20_URLS&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;&lt;/span&gt;


title &lt;span class=&#34;s2&#34;&gt;&amp;quot;top 20 URLS excluding POST data&amp;quot;&lt;/span&gt;
&lt;span class=&#34;nv&#34;&gt;TOP_20_URLS_WITHOUT_POST&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;`tail  -&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;$HOW_MANY_ROWS&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;$ACCESS_LOG&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt; | awk -F&amp;quot;&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;[&lt;/span&gt; ?&lt;span class=&#34;o&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot; &amp;#39;{freq[&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;$7&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;]++} END {for (x in freq) {print freq[x], x}}&amp;#39; | sort -rn | head -20`&amp;quot;&lt;/span&gt;
&lt;span class=&#34;nb&#34;&gt;echo&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;$TOP_20_URLS_WITHOUT_POST&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;&lt;/span&gt;


title &lt;span class=&#34;s2&#34;&gt;&amp;quot;top 20 IPs&amp;quot;&lt;/span&gt;
&lt;span class=&#34;nv&#34;&gt;TOP_20_IPS&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;`tail  -&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;$HOW_MANY_ROWS&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;$ACCESS_LOG&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt; | awk &amp;#39;{freq[&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;$1&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;]++} END {for (x in freq) {print freq[x], x}}&amp;#39; | sort -rn | head -20`&amp;quot;&lt;/span&gt;
&lt;span class=&#34;nb&#34;&gt;echo&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;$TOP_20_IPS&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;&lt;/span&gt;

title &lt;span class=&#34;s2&#34;&gt;&amp;quot;top 20 user agents&amp;quot;&lt;/span&gt;
&lt;span class=&#34;nv&#34;&gt;TOP_20_USER_AGENTS&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;`tail  -&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;$HOW_MANY_ROWS&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt; &lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;$ACCESS_LOG&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt; | cut -d\  -f12- | sort | uniq -c | sort -rn | head -20`&amp;quot;&lt;/span&gt;
&lt;span class=&#34;nb&#34;&gt;echo&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;$TOP_20_USER_AGENTS&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;&lt;/span&gt;


title &lt;span class=&#34;s2&#34;&gt;&amp;quot;IP Addresses for Top 3 User Agents&amp;quot;&lt;/span&gt;

&lt;span class=&#34;k&#34;&gt;for&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;((&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;I&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;1&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; I&amp;lt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;3&lt;span class=&#34;p&#34;&gt;;&lt;/span&gt; I++&lt;span class=&#34;o&#34;&gt;))&lt;/span&gt;
&lt;span class=&#34;k&#34;&gt;do&lt;/span&gt;
    &lt;span class=&#34;nv&#34;&gt;UA&lt;/span&gt;&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;`echo &amp;quot;&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;$TOP_20_USER_AGENTS&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot; | head -n &lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;$I&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt; | tail -n 1 | awk &amp;#39;{&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;$1&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;=&amp;quot;&amp;quot;; print &lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;$0&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;}&amp;#39;`&amp;quot;&lt;/span&gt;
    &lt;span class=&#34;nb&#34;&gt;echo&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;$UA&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;&lt;/span&gt;
    &lt;span class=&#34;nb&#34;&gt;echo&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;quot;~~~~~~~~~~~~~~~~~~&amp;quot;&lt;/span&gt;
    ip_addresses_by_user_agent &lt;span class=&#34;s2&#34;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&#34;nv&#34;&gt;$UA&lt;/span&gt;&lt;span class=&#34;s2&#34;&gt;&amp;quot;&lt;/span&gt;
    &lt;span class=&#34;nb&#34;&gt;echo&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;quot;&lt;/span&gt;
&lt;span class=&#34;s2&#34;&gt;    &amp;quot;&lt;/span&gt;
&lt;span class=&#34;k&#34;&gt;done&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Wget For Beginners</title>
      <link>https://edmondscommerce.github.io/linux/wget-for-beginners.html</link>
      <pubDate>Mon, 19 Nov 2012 08:02:25 +0000</pubDate>
      
      <guid>https://edmondscommerce.github.io/linux/wget-for-beginners.html</guid>
      <description>&lt;h3&gt;What is Wget in general ?&lt;/h3&gt;

&lt;p&gt;Wget is a free utility for non-interactive download of file from the web. The user doesnâ€™t need to login system every time &lt;a href=&#34;http://www.gnu.org/software/wget/&#34; rel=&#34;nofollow&#34;&gt;Wget&lt;/a&gt; can download the entire web page or mirroring the entire web page. If download crashed or stop for various reason Wget will start download again from where it stopped. It is highly recommended for downloading file from web with slow network connections.&lt;/p&gt;

&lt;h2&gt;How to Use Wget ? &lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;code class=&#34;language-php&#34; data-lang=&#34;php&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&#34;x&#34;&gt;wget -t 10 www.google.com&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;If network connection fails Wget will to try to reconnect 20 times in default.With -t command we can specify how many times it need to  reconnect.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;code class=&#34;language-php&#34; data-lang=&#34;php&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&#34;x&#34;&gt;wget -p --convert-links -r www.google.com -o logfile&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;This command will download the site -p and &amp;ndash;convert will make sure all linked files are linked to downloaded document such as images and external links it enables complete offline viewing. Log file can be enabled with -O command to view the output message.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;code class=&#34;language-php&#34; data-lang=&#34;php&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&#34;x&#34;&gt;wget --spider --force-html www.google.com&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;&amp;ndash;spider Will check the webpage is existent or not.
&amp;ndash;force  It will enforce the file type that have to be downloaded.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;code class=&#34;language-php&#34; data-lang=&#34;php&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&#34;x&#34;&gt;wget -u mozilla www.google.com&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;some site access allowed to certain user &lt;a href=&#34;http://www.user-agents.org/&#34; rel=&#34;nofollow&#34;&gt;agents&lt;/a&gt;. So to access with certain user agent you can use this command.&lt;/p&gt;

&lt;p&gt;Ftp connection download is achieved by&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre&gt;&lt;code class=&#34;language-php&#34; data-lang=&#34;php&#34;&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&#34;x&#34;&gt;wget -r ftp://username:password@ftp.example.com&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Format searched log files</title>
      <link>https://edmondscommerce.github.io/bash/format-searched-log-files.html</link>
      <pubDate>Thu, 16 Aug 2012 15:45:56 +0000</pubDate>
      
      <guid>https://edmondscommerce.github.io/bash/format-searched-log-files.html</guid>
      <description>&lt;p&gt;When you are monitoring a log file you may want to narrow it down, and format the results. This simple one line command will break up the output from a log file to make it easier to quickly read&lt;/p&gt;

&lt;p&gt;&lt;code&gt;
tail -f /path/to/log | grep &amp;ldquo;search term&amp;rdquo; | sed &amp;rsquo;s/(.*)/&amp;mdash;&amp;mdash;&amp;mdash;-\n\1\n&amp;mdash;&amp;mdash;&amp;mdash;-/&amp;rsquo;
&lt;/code&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>MySQL Server Gone Away - Large Packets the Issue?</title>
      <link>https://edmondscommerce.github.io/mysql/mysql-server-gone-away-large-packets-the-issue.html</link>
      <pubDate>Wed, 15 Aug 2012 12:58:23 +0000</pubDate>
      
      <guid>https://edmondscommerce.github.io/mysql/mysql-server-gone-away-large-packets-the-issue.html</guid>
      <description>&lt;p&gt;If you are scratching your head trying to figure out why you keep getting MySQL server gone away error messages despite the fact you have bumped up all the timeout etc configurations to high values then this could be your solution.&lt;/p&gt;

&lt;p&gt;MySQL will also give you this error if you try to send a packet that is larger than the packet size defined. We had an application that was using MariaDB which has a default max allowed packet of 16M by default.&lt;/p&gt;

&lt;p&gt;The application in question was sending large amounts of data to be stored and so the solution to the gone away issues was simply to increase the max_allowed_packet configuration in my.cnf, restart the mysql daemon and the problems are sorted.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>How 10,000 Geeks break the UK&#39;s biggest distributors, RS and Farnell (Raspberry Pi)</title>
      <link>https://edmondscommerce.github.io/ecommerce/how-10000-geeks-break-the-uks-biggest-distributors-rs-and-farnell-raspberry-pi.html</link>
      <pubDate>Wed, 29 Feb 2012 06:50:10 +0000</pubDate>
      
      <guid>https://edmondscommerce.github.io/ecommerce/how-10000-geeks-break-the-uks-biggest-distributors-rs-and-farnell-raspberry-pi.html</guid>
      <description>&lt;p&gt;So, 6am this morning (Wed 29 2012), a bunch of geeks eagerly awaiting the Raspberry Pi have killed the websites of both RS Components and Farnell&lt;/p&gt;

&lt;p&gt;Why, you ask? Because the servers were not prepared for the influx of visitors that such an announcement brings.  Raspberry Pi themselves (a UK Foundation, like a Charity) were well prepared and are presently serving up a static page.&lt;/p&gt;

&lt;p&gt;So, if you&amp;rsquo;re running an eCommerce site, like Magento or osCommerce, and are warned by one of your suppliers, or your manufacturing department that you&amp;rsquo;re launching a new product, be prepared.  Raspberry Pi certainly were and provided their suppliers with the information that they would be getting a huge amount of traffic, unfortunately those suppliers didn&amp;rsquo;t listen hard enough.&lt;/p&gt;

&lt;p&gt;This is to all intents a DDoS attack, but not an intentional attack.  Well done for keeping everyone updated via twitter though @raspberrypi, we know it isn&amp;rsquo;t your fault.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>VirtualBox Images For Download of Various Linux and Other Open Source Systems</title>
      <link>https://edmondscommerce.github.io/virtualbox/virtualbox-images-for-download-of-various-linux-and-other-open-source-systems.html</link>
      <pubDate>Thu, 23 Feb 2012 12:42:17 +0000</pubDate>
      
      <guid>https://edmondscommerce.github.io/virtualbox/virtualbox-images-for-download-of-various-linux-and-other-open-source-systems.html</guid>
      <description>&lt;p&gt;Virtualbox is an amazing tool for testing and developing on various systems.&lt;/p&gt;

&lt;p&gt;Now it&amp;rsquo;s even easier as there is a nicely organised repository of clean VirtualBox images you can download and get running with in a matter of minutes.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://virtualboxes.org/&#34;&gt;http://virtualboxes.org/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;We are going to use them for some &lt;a href=&#34;http://www.edmondscommerce.co.uk&#34;&gt;Magento&lt;/a&gt; performance optimisation testing on various platforms as we perfect our &lt;a href=&#34;http://www.edmondscommerce.co.uk/magento/magento-optimised-hosting/&#34;&gt;Magento Server Optimisation&lt;/a&gt; skills and try to benchmark some new ideas.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>How To Extract Files from Plesk Backups</title>
      <link>https://edmondscommerce.github.io/plesk/how-to-extract-files-from-plesk-backups.html</link>
      <pubDate>Mon, 30 Jan 2012 17:25:16 +0000</pubDate>
      
      <guid>https://edmondscommerce.github.io/plesk/how-to-extract-files-from-plesk-backups.html</guid>
      <description>&lt;p&gt;So you have a plesk backup file and you want to extract and open the files from it? No problem. This blog will show you how.&lt;/p&gt;

&lt;p&gt;The file is a mime file. The &amp;ldquo;mpack&amp;rdquo; package will let you unpack it.&lt;/p&gt;

&lt;p&gt;First we install the &amp;ldquo;mpack&amp;rdquo; package&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo apt-get install mpack
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Let&amp;rsquo;s imagine your file is called &amp;ldquo;pleskDump.gz&amp;rdquo;. If it doesn&amp;rsquo;t have a &amp;ldquo;.gz&amp;rdquo; at the end it might not be bad to add it as some environments will complain if it&amp;rsquo;s absent.&lt;/p&gt;

&lt;p&gt;Next we unzip the file&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;gunzip pleskDump.gz
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now let&amp;rsquo;s un-mime the file&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;mkdir pleskDumpOutput
cd pleskDumpOutput
cat ../pleskDump | munpack
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now we have a bunch of regular tar files, but be careful, if we just extract them, the root folders will not be recreated. In order to keep things together, it&amp;rsquo;s best now to look at the output you have. Let&amp;rsquo;s take a made up example:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;example-domain.com.httpdocs
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note that the format is essentially domain.rootfolder or in another way, the tar files have your domain name they are archiving, then a dot, then the name of the root folder they made up. We take that root folder and create it like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;mkdir rootfolder
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now let&amp;rsquo;s apply that idea to our previoud example; &amp;ldquo;example-domain.com&amp;rdquo;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;mkdir httpdocs
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;It&amp;rsquo;s time to untar into the folder you just created, let&amp;rsquo;s assume you just created &amp;ldquo;httpdocs&amp;rdquo;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;tar -xvf example-domain.com.httpdocs -C httpdocs
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;There you go! You can now output any of the folders you wanted.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Writing Robust Bash Shell Scripts</title>
      <link>https://edmondscommerce.github.io/bash/writing-robust-bash-shell-scripts.html</link>
      <pubDate>Sun, 15 Jan 2012 19:28:02 +0000</pubDate>
      
      <guid>https://edmondscommerce.github.io/bash/writing-robust-bash-shell-scripts.html</guid>
      <description>&lt;p&gt;Found this great article on writing bash scripts defensively. Glad to say most of the advice we are already following with our bash scripts however there are a couple of new things I intend to roll in on Monday.&lt;/p&gt;

&lt;p&gt;Recommended reading for anyone scripting in Bash.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://www.davidpashley.com/articles/writing-robust-shell-scripts.html&#34;&gt;http://www.davidpashley.com/articles/writing-robust-shell-scripts.html&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Server Migration - Synch Cron Tab via SSH</title>
      <link>https://edmondscommerce.github.io/linux/server-migration-synch-cron-tab-via-ssh.html</link>
      <pubDate>Wed, 04 Jan 2012 17:54:14 +0000</pubDate>
      
      <guid>https://edmondscommerce.github.io/linux/server-migration-synch-cron-tab-via-ssh.html</guid>
      <description>&lt;p&gt;If you are handling a server migration and would like to have a scripted way to copy the crontab from one machine to the other then you might like this little snippet.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;

ssh -p2020 root@123.123.123.123 &#39;crontab -l&#39; | crontab -


&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This will get the contents of the root crontab from one server and apply it to the current server, replacing any current cron tab settings.&lt;/p&gt;

&lt;p&gt;This is nice if you want to have a repeatable server resynch process whilst you are migrating and the crontab on the old server may change&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Keep ssh alive</title>
      <link>https://edmondscommerce.github.io/linux/keep-ssh-alive.html</link>
      <pubDate>Fri, 18 Nov 2011 15:05:20 +0000</pubDate>
      
      <guid>https://edmondscommerce.github.io/linux/keep-ssh-alive.html</guid>
      <description>&lt;p&gt;&lt;div class=&#34;oldpost&#34;&gt;&lt;h4&gt;This is post is now quite old and the the information it contains may be out of date or innacurate.&lt;/h4&gt;
&lt;p&gt;
If you find any errors or have any suggestions to update the information &lt;a href=&#34;http://edmondscommerce.github.io/contact-us/index.html&#34;&gt;please let us know&lt;/a&gt;
or &lt;a href=&#34;https://github.com/edmondscommerce/edmondscommerce.github.io&#34;&gt;create a pull request on GitHub&lt;/a&gt;
&lt;/p&gt;
&lt;/div&gt;
Due to our load-balancing router being a little &amp;ldquo;harsh&amp;rdquo; on closing connections that appear to be unused, ssh often sits and hangs.&lt;/p&gt;

&lt;p&gt;Using a standard ssh config option though we can set a server keep-alive (on the ssh client, which also keeps sshfs alive)&lt;/p&gt;

&lt;p&gt;In the &amp;ldquo;~/.ssh/config&amp;rdquo; file, simply adding the following keeps the connection sending a small packet every 30 seconds :-
&lt;code&gt;ServerAliveInterval=30&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;I&amp;rsquo;m sure windows clients like puTTY also have a similar option, and on a mac, it should be the same file as Linux.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>mysql_dump not restored correctly 1064: USING BTREE &#43; fix</title>
      <link>https://edmondscommerce.github.io/mysql/mysql_dump-not-restored-correctly-1064-using-btree-fix.html</link>
      <pubDate>Thu, 10 Nov 2011 18:30:05 +0000</pubDate>
      
      <guid>https://edmondscommerce.github.io/mysql/mysql_dump-not-restored-correctly-1064-using-btree-fix.html</guid>
      <description>&lt;p&gt;&lt;div class=&#34;oldpost&#34;&gt;&lt;h4&gt;This is post is now quite old and the the information it contains may be out of date or innacurate.&lt;/h4&gt;
&lt;p&gt;
If you find any errors or have any suggestions to update the information &lt;a href=&#34;http://edmondscommerce.github.io/contact-us/index.html&#34;&gt;please let us know&lt;/a&gt;
or &lt;a href=&#34;https://github.com/edmondscommerce/edmondscommerce.github.io&#34;&gt;create a pull request on GitHub&lt;/a&gt;
&lt;/p&gt;
&lt;/div&gt;
When migrating one server to another you often hit bumps in the road.  Todays was transferring a database from one server to another.&lt;/p&gt;

&lt;p&gt;During this standard procedure I found that the restored database was missing a few tables.  Irritating as Magento doesn&amp;rsquo;t like missing tables.&lt;/p&gt;

&lt;p&gt;Digging down into the backup and extracting the first missing table I was able to replicate the error which came out as
&lt;code&gt;1064 - You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near &amp;lsquo;USING BTREE&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;I eventually found out that some versions of mysql 5.1 export a dump file that contains mysql5.1 specific features and loading the file into mysql 5.0 will not work.&lt;/p&gt;

&lt;p&gt;The solution is a little frustrating but if you run the command with the &amp;ndash;compatible=mysql40 switch, the dump file extracts fine :-
&lt;code&gt;mysqldump &amp;ndash;compatible=mysql40&lt;/code&gt;
Don&amp;rsquo;t ask me why there&amp;rsquo;s no &amp;ndash;compatible=mysql50 flag.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>